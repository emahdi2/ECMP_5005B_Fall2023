---
title: "**Transformations in Regression**"
author: "Esam Mahdi"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: cerulean
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, root.dir = "C:/Users/mahdi/Documents/GitHub/Topic2", warning=FALSE, message=FALSE)
```

------------------------------------------------------------------------

## R-ode is for modelling defective rates

First, we read the data and then obtain a scatter plot matrix of the random variable Defects, $Y$, against the independent variables.
We notice that the relationship is between $Y$ and each one of $X'$s looks like quadratic and not linear.
Thus, transformation might be needed.

```{r}
defects <- read.table("https://gattonweb.uky.edu/sheather/book/docs/datasets/defects.txt", header=TRUE)
attach(defects)
# A scatter plot matrix of the defects data
pairs(Defective ~ Temperature + Density + Rate)
```

After that we fit the regression line
$$Y = \beta_0 + \beta_1 X_1+ \beta_2 X_2+ \beta_3 X_3+\varepsilon$$ where

-    $Y=$ Defective.
-    $X_1=$ Temperature.
-    $X_2=$ Density.
-    $X_3=$ Rate.

Then, we plot the the standardized residuals against each one of the predictors.
Note that we add a locally-weighted polynomial regression *LOWESS to the plots to help us in visualizing the relationship between the standardized residuals and each of the predictor variables*.

```{r}
m1 <- lm(Defective ~ Temperature  + Density + Rate)
# Plots of the standardized residuals against X1,X2,X3
par(mfrow=c(2,2))
StanRes1 <- rstandard(m1)
plot(Temperature, StanRes1, ylab = "Standardized Residuals")
lines(lowess(StanRes1 ~ Temperature), col = 2)

plot(Density, StanRes1, ylab = "Standardized Residuals")
lines(lowess(StanRes1 ~ Density), col = 2)

plot(Rate, StanRes1, ylab = "Standardized Residuals")
lines(lowess(StanRes1 ~ Rate), col = 2)

plot(m1$fitted.values, StanRes1, ylab = "Standardized Residuals",xlab="Fitted Values")
lines(lowess(StanRes1 ~ m1$fitted.values), col = 2)
```

The plots of standardized residuals against each predictor and the fitted values clearly do not produce random scatters (looks like parabola shape).
Thus the fitted model is not valid and it is natural to consider a transformation of Y.

Now, to check whether the relationship between the dependent variable and independents is quadratic, [ i.e., something like $Y = g(\beta_0 + \beta_1 X_1+ \beta_2 X_2+ \beta_3 X_3+\varepsilon)\approx [\beta_0 + \beta_1 X_1+ \beta_2 X_2+ \beta_3 X_3+\varepsilon]^2$, we plot the values of Y against their fitted values with a straight line and a quadratic curve.
The plots confirm that we have a quadratic parabola and not linear.

```{r}
# Plot of Y against fitted values with a straight line and a quadratic curve
par(mfrow = c(1,1))
fit1 <- m1$fitted.values
m2 <- lm(Defective ~ fit1 + I(fit1^2))
plot(fit1, Defective, xlab = "Fitted Values")
fitnew <- seq(-15,60, len = 76)
lines(fitnew,predict(m2, newdata = data.frame(fit1 = fitnew)))
abline(lsfit(m1$fitted.values, Defective), lty = 2)
```

### Inverse response plots

We use the function \texttt{invResPlot()} from the package **car** to estimate $\lambda$ and get $\hat{\lambda}=0.44$.

```{r}
library("car")
invResPlot(m1)
```

The inverse response plots suggest that we shall transform Y by taking the square root and thus consider the following model $$\sqrt{Y} =g^{-1}(Y) = \beta_0 + \beta_1 X_1+ \beta_2 X_2+ \beta_3 X_3+\varepsilon$$

### Box-Cox transformation

Now, we apply the Box-Cox transformation test using the function \texttt{boxcox()} from the **R** package **MASS** to estimate the parameter $\lambda$ that maximizes the log-likelihood.

```{r}
#inverse.response.plot(m1,key=TRUE)
# Log-likelihood for the Box-Cox transformation method
library(MASS)
bc <- boxcox(m1, lambda = seq(0.3, 0.65, length = 20))
bc$x[which.max(bc$y)] ## to get the value of lambda
```

We conclude from the Box-Cox procedures that $\lambda=0.45\approx 0.5$.
Thus, it is reasonable to model the square root of Defective variable based on a linear combination of $X_1, X_2, X_3$.
i.e.,

$$\sqrt{Y} = \beta_0 + \beta_1 X_1+ \beta_2 X_2+ \beta_3 X_3+\varepsilon$$ The next step is to examine whether the transformation of $Y$ using square root makes the relationship between $\sqrt{Y}$ and each one of $X_1, X_2, X_3$ liner?

```{r}
# Plots of square root of Y against each predictor
par(mfrow = c(2, 2))
plot(Temperature, sqrt(Defective), ylab = expression(sqrt(Defective)))
plot(Density, sqrt(Defective), ylab = expression(sqrt(Defective)))
plot(Rate, sqrt(Defective), ylab = expression(sqrt(Defective)))
```

It is evident from the plots that the relationship between $\sqrt{Y}$ and each predictor is more linear than the relationship between Y and each predictor.

Now, we fit the model after transformation and perform diagnostic checks using the standardized residuals.

```{r}
# Fit the model after transformation
mt <- lm(sqrt(Defective) ~ Temperature + Density + Rate)
# Plots of the standardized residuals (Diagnostic checks)
par(mfrow = c(2, 2))
StanRest <- rstandard(mt)
plot(Temperature, StanRest, ylab = "Standardized Residuals")
plot(Density, StanRest, ylab = "Standardized Residuals")
plot(Rate, StanRest, ylab = "Standardized Residuals")
plot(mt$fitted.values, StanRest, ylab = "Standardized Residuals", xlab = "Fitted Values")
# Plots of square root of Y against fitted values with a straight line added
par(mfrow = c(1, 1))
plot(mt$fitted.values, sqrt(Defective), xlab = "Fitted Values", ylab = expression(sqrt(Defective)))
abline(lsfit(mt$fitted.values, sqrt(Defective)))
# Use diagnostic plots provided by R
par(mfrow = c(2, 2))
plot(mt)
```

The diagnostic plots show that the transformation model is a valid model for the data and our model from the regression output summary is

$$\sqrt{\widehat{\text{Defective}}} = 5.59297 + 1.56516 \times \text{Temperature} -0.29166 \times \text{Density} + 0.01290 \times \text{Rate}$$

```{r}
# Final Regression output 
summary(mt)
```

Note that the variable `Rate` is not statistically significant, thus not supporting Ole's theory.
On the other hand, the coefficient of `Density` is statistically significantly negative, which supports the Ole's theory that the increasing in the `density value` leads to lowering the `defect rate`.
However, the `Rate` (which is related to the other two predictors) still needs to be considered when adjustments are made to one or both of the statistically significant predictors.

```{r}
detach(defects)
```
